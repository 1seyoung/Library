# 1. 데이터 수집 및 수집 개념

## 1.1 데이터 수집의 정의

데이터 수집은 다양한 출처로부터 데이터를 추출하여 원하는 형태로 가공 및 저장하는 과정입니다. 데이터의 출처는 크게 아래와 같이 나눌 수 있습니다.

• **DB(데이터베이스)**: MySQL, PostgreSQL, Oracle과 같은 RDBMS, 또는 MongoDB, Cassandra와 같은 NoSQL DB로부터 데이터를 가져올 수 있습니다.
• **API(응용 프로그램 인터페이스)**: RESTful API나 GraphQL API 등, 외부 시스템으로부터 데이터를 호출하는 방식입니다.
• **파일 시스템**: CSV, Excel, JSON 등의 파일로 저장된 데이터를 로컬 또는 원격 파일 시스템에서 추출할 수 있습니다.

이 과정에서 데이터는 원시 상태로 추출되며, 추출된 데이터를 분석, 활용 가능하게 하기 위해서는 적절한 가공이 필요합니다.

## 1.2 Python을 이용한 데이터 수집

Python은 데이터 수집에 매우 유용한 라이브러리와 도구를 제공합니다. 주로 사용되는 두 가지 라이브러리는 pandas와 requests입니다.

1. **pandas**: Python의 대표적인 데이터 처리 라이브러리로, 데이터베이스나 파일로부터 데이터를 쉽게 가져올 수 있습니다.
	- pandas.read_sql(): SQL 쿼리를 사용해 데이터베이스로부터 데이터를 읽어오는 함수.
	- pandas.read_csv(): CSV 파일로부터 데이터를 읽어오는 함수.

2. **requests**: API를 호출하여 데이터를 가져오는 데 사용하는 라이브러리입니다. RESTful API나 웹 서비스로부터 데이터를 추출할 때 자주 사용됩니다.
	- requests.get(): HTTP GET 요청을 보내어 데이터를 가져오는 함수.
	- requests.post(): HTTP POST 요청을 보내어 데이터를 서버에 제출하는 함수.

## 1.3 데이터 수집의 예시

1. **데이터베이스에서 데이터 추출**

```python
import pandas as pd
import sqlite3

# SQLite 데이터베이스 연결
conn = sqlite3.connect('example.db')

# SQL 쿼리를 사용하여 데이터베이스에서 데이터 읽기
df = pd.read_sql_query("SELECT * FROM users", conn)

print(df.head())
```


2. **API로부터 데이터 수집**

```python
import requests

# API로부터 데이터 가져오기
url = "https://api.example.com/data"
response = requests.get(url)

# 응답 데이터를 JSON 형식으로 변환
data = response.json()

print(data)
```


## 1.4 ETL 프로세스 (Extract, Transform, Load)

ETL은 데이터 수집 과정에서 필수적인 절차입니다. ETL은 데이터를 추출하고(Extract), 변환하며(Transform), 데이터베이스나 데이터 웨어하우스에 적재하는(Load) 프로세스를 의미합니다.

• **Extract(추출)**: 데이터베이스, API, 파일 등으로부터 데이터를 가져오는 단계입니다.
• **Transform(변환)**: 추출된 데이터를 필요한 형태로 가공하는 단계입니다. 여기에는 데이터 정제, 필터링, 변환 등이 포함됩니다.
• **Load(적재)**: 가공된 데이터를 최종 저장소(데이터 웨어하우스, 데이터베이스, 파일 등)에 적재하는 단계입니다.

  

## 1.5 ETL 과정의 세부 사항


1. **추출(Extract)**

• 데이터 소스에서 데이터를 선택하고 추출하는 첫 번째 단계입니다. 여기서 가장 중요한 것은 데이터의 정확성 및 일관성을 유지하며 추출하는 것입니다. API나 데이터베이스와 같은 소스의 연결성을 고려하여 적절한 데이터 양을 추출하는 것이 중요합니다.

• 예시:
```python
# API로부터 JSON 데이터를 추출하는 과정
import requests
url = "https://api.example.com/data"
response = requests.get(url)
data = response.json()
```

2. **변환(Transform)**

• 데이터가 원하는 형태로 사용되기 위해서는 변환 작업이 필요합니다. 변환 작업에는 불필요한 열 제거, 데이터 형식 변환, 결측값 처리, 데이터 정렬 등이 포함될 수 있습니다.

• 예시:

```python
# 결측값을 처리하고 데이터 형식을 변환하는 예시
import pandas as pd
df = pd.DataFrame(data)

# 결측값 처리
df.fillna(0, inplace=True)

# 열 변환
df['date'] = pd.to_datetime(df['date'])
```


3. **적재(Load)**

• 가공된 데이터를 최종 저장소에 적재하는 단계입니다. 이 저장소는 데이터베이스, 데이터 웨어하우스 또는 파일 시스템일 수 있습니다.

• 예시:
```python
# 변환된 데이터를 SQLite 데이터베이스에 적재하는 예시
conn = sqlite3.connect('example.db')
df.to_sql('processed_data', conn, if_exists='replace')
```

## 1.6 데이터 수집의 중요성

• **데이터 분석의 기반**: 데이터 수집은 분석의 출발점입니다. 수집된 데이터의 품질과 양이 이후 분석 결과에 큰 영향을 미칩니다.
• **의사결정 지원**: 올바른 데이터를 수집함으로써 비즈니스 의사결정을 더 정확하게 내릴 수 있습니다.
• **자동화된 데이터 흐름 구축**: 데이터 수집 과정이 자동화되면, 실시간 데이터를 기반으로 한 시스템 운영 및 예측 분석이 가능해집니다.


**1.7 데이터 수집에서의 고려 사항**

• **데이터의 품질**: 수집되는 데이터가 신뢰할 수 있고 정확한지 검토해야 합니다.
• **데이터 보안**: 데이터를 수집하는 과정에서 개인정보나 민감한 정보가 포함되어 있다면, 보안 관리를 철저히 해야 합니다.
• **데이터 소스의 다양성**: 가능한 많은 소스에서 데이터를 수집하여 분석의 폭을 넓히는 것이 중요합니다.
  
## 결론

데이터 수집은 데이터 기반 의사결정과 분석의 기초가 됩니다. Python을 활용한 효율적인 데이터 수집 방법을 이해하고, ETL 프로세스를 잘 설계하면 보다 높은 품질의 데이터를 확보할 수 있습니다. 이 과정에서 데이터의 품질과 보안을 지속적으로 모니터링하는 것이 중요하며, 수집된 데이터를 바탕으로 향후 분석 작업이 이루어지기 때문에 그 중요성은 매우 큽니다.

---

# 데이터 파이프라인 구축 개념

## 2.1 데이터 파이프라인의 정의

**데이터 파이프라인**은 데이터를 원천으로부터 최종 목적지까지 자동으로 전달하는 일련의 프로세스를 의미합니다. 이 과정에서 데이터는 수집, 처리, 저장, 그리고 분석을 위해 전달됩니다. 데이터 파이프라인의 핵심 목표는 데이터를 지속적이고 체계적으로 처리하며, 데이터의 흐름을 자동화함으로써 운영의 효율성을 높이는 데 있습니다.

• **수집**: 데이터베이스, API, 파일 등 다양한 소스에서 데이터를 수집.

• **처리**: 수집된 데이터를 정제, 변환, 필터링하여 필요한 형식으로 가공.

• **전달**: 가공된 데이터를 데이터 웨어하우스, 데이터베이스, 파일 시스템 등으로 이동시키는 과정.

데이터 파이프라인을 성공적으로 구축하면, 데이터의 흐름을 자동화하여 사람이 개입하지 않고도 신속하고 효율적으로 데이터를 처리할 수 있습니다.

  

## 2.2 Apache Airflow 개념


**Apache Airflow**는 데이터를 처리하는 파이프라인을 관리하는 오픈소스 워크플로우 매니저입니다. 복잡한 데이터 워크플로우를 자동화하고, 다양한 작업의 의존성을 체계적으로 관리할 수 있게 해줍니다. Airflow는 **DAG(Directed Acyclic Graph)** 구조를 사용해 작업 간의 의존성을 정의하고, 스케줄링된 시간에 맞춰 파이프라인을 실행합니다.

  
• **DAG(Directed Acyclic Graph)**: DAG는 방향성이 있는 비순환 그래프로, 작업들이 순차적으로 실행되어야 하는 순서를 정의합니다. 하나의 작업은 다른 작업에 의존할 수 있으며, 이 의존 관계에 따라 실행 흐름이 결정됩니다.

• **Task**: DAG 안에서 실행되는 개별 작업입니다. Task는 주로 특정 데이터 처리 작업(예: 데이터 추출, 데이터 변환 등)을 수행합니다.

• **Operator**: Airflow에서 Task를 정의하는 요소입니다. BashOperator, PythonOperator, MySqlOperator 등 다양한 작업 유형에 맞춘 Operator가 있습니다.

  

## 2.3 GCP Cloud Composer 개념

  

**GCP Cloud Composer**는 Google Cloud Platform(GCP)에서 제공하는 관리형 Airflow 서비스입니다. Cloud Composer를 사용하면 Apache Airflow를 직접 설치하거나 관리하지 않고도, GCP 환경에서 복잡한 데이터 파이프라인을 쉽게 운영할 수 있습니다. 이를 통해 Airflow 인프라를 손쉽게 확장하거나 모니터링할 수 있으며, GCP의 다른 서비스들과의 통합이 용이합니다.

  

• **완전 관리형 서비스**: Cloud Composer는 GCP가 Airflow 인프라를 관리해주기 때문에 사용자는 파이프라인 개발과 운영에만 집중할 수 있습니다.

• **GCP 서비스와의 통합**: GCP의 BigQuery, Cloud Storage, Pub/Sub과 같은 서비스와 쉽게 통합하여 데이터 파이프라인을 구성할 수 있습니다.

• **확장성**: 사용자가 증가하거나 처리량이 늘어날 때 자동으로 확장할 수 있는 유연한 인프라를 제공합니다.

  
---
# 3. 데이터 파이프라인 개선 개념

  

## 3.1 데이터 파이프라인 개선 도구

  

데이터 파이프라인의 성능을 지속적으로 개선하고 문제를 사전에 감지하기 위해 다양한 도구가 사용됩니다. 이러한 도구들은 데이터의 품질을 모니터링하고, 데이터 전달을 최적화하며, 실시간으로 데이터 파이프라인의 상태를 추적하는 데 중요한 역할을 합니다.

  

## 3.2 DQP(Data Quality Platform) 개념

  

**DQP(Data Quality Platform)**은 데이터 품질을 관리하고 평가하는 플랫폼입니다. 데이터 파이프라인에서 수집되고 처리된 데이터가 정확하고 신뢰할 수 있는지 평가하며, 품질이 낮은 데이터가 전달되지 않도록 필터링하는 역할을 합니다.


• **데이터 품질 관리**: DQP는 데이터의 일관성, 정확성, 완전성, 중복성 등을 자동으로 평가하고, 품질 문제가 발생할 경우 경고를 보내거나 문제를 해결합니다.

• **자동화된 품질 검사**: 파이프라인의 각 단계에서 데이터가 올바르게 처리되고 있는지 자동으로 검토하는 시스템을 제공합니다.

• **중요성**: 데이터의 품질이 낮을 경우 분석 결과의 신뢰성에 문제가 발생할 수 있기 때문에, 데이터 품질 관리는 매우 중요합니다.

  

## 3.3 DDP(Data Delivery Platform) 개념


**DDP(Data Delivery Platform)**는 데이터를 목적지로 안정적이고 효율적으로 전달하는 플랫폼입니다. 다양한 소스로부터 데이터를 수집하고, 여러 목적지(데이터 웨어하우스, 분석 시스템 등)로 전달하는 역할을 합니다.


• **안정적 전달**: DDP는 네트워크 문제나 시스템 오류가 발생하더라도 데이터를 안전하게 전달하는 기능을 제공합니다.

• **다양한 소스와 목적지 지원**: 다양한 데이터 소스(API, 데이터베이스, 파일 등)로부터 데이터를 수집하고, 이를 적절한 목적지로 전달합니다.

• **데이터 전달 최적화**: 데이터를 전달하는 과정에서 중복을 피하고, 불필요한 데이터 이동을 최소화하여 성능을 최적화합니다.

  

## 3.4 Data Observability 개념

  

**Data Observability**는 데이터 파이프라인의 성능과 상태를 실시간으로 모니터링하는 도구입니다. 데이터 흐름에서 발생하는 문제를 감지하고, 빠르게 대응할 수 있도록 지원합니다. 이는 데이터 파이프라인의 안정성을 높이고, 운영 효율성을 극대화하는 데 중요한 역할을 합니다.

• **실시간 모니터링**: 데이터가 예상대로 흐르고 있는지, 데이터 처리 성능이 저하되지는 않았는지 실시간으로 모니터링합니다.

• **문제 감지 및 대응**: 데이터 손실, 지연, 오류 등의 문제가 발생했을 때 이를 감지하고 빠르게 해결할 수 있는 알림 시스템을 제공합니다.

• **향상된 가시성**: 데이터 파이프라인 전반의 흐름을 한눈에 볼 수 있어, 운영자가 파이프라인을 효과적으로 관리할 수 있습니다.

  

## 결론

데이터 파이프라인은 데이터를 자동으로 수집, 처리, 전달하는 중요한 시스템입니다. Apache Airflow와 같은 워크플로우 관리 도구는 데이터 파이프라인을 쉽게 설계하고 운영할 수 있게 도와주며, Cloud Composer는 GCP 환경에서 이러한 파이프라인을 관리하는 데 편리한 서비스를 제공합니다. 또한, 데이터 파이프라인의 성능과 품질을 유지하기 위해 DQP, DDP, Data Observability와 같은 도구들이 활용되며, 이들은 데이터 품질, 안정적 전달, 실시간 모니터링을 통해 파이프라인의 성능을 최적화합니다.