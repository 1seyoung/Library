### 초록
이 논문에서 우리는 의미론적 분할에서 문맥 집계 문제를 연구합니다. 픽셀의 레이블이 그 픽셀이 속한 객체의 카테고리라는 점에 동기를 얻어, 우리는 해당 객체 클래스의 표현을 활용하여 픽셀을 특성화하는 간단하지만 효과적인 접근법인 객체-문맥 표현을 제시합니다. 먼저, 우리는 ground-truth 분할의 감독하에 객체 영역을 학습합니다. 두 번째로, 우리는 객체 영역에 속하는 픽셀들의 표현을 집계하여 객체 영역 표현을 계산합니다. 마지막으로, 우리는 각 픽셀과 각 객체 영역 간의 관계를 계산하고, 각 픽셀의 표현을 모든 객체 영역 표현의 가중 집계인 객체-문맥 표현으로 향상시킵니다. 우리는 다양한 벤치마크에서 우리의 방법이 경쟁력 있는 성능을 달성함을 경험적으로 입증합니다: Cityscapes, ADE20K, LIP, PASCAL-Context 및 COCO-Stuff. 우리의 제출인 “HRNet + OCR + SegFix”는 ECCV 2020 제출 마감일까지 Cityscapes 리더보드에서 1위를 달성했습니다. 코드는 다음에서 이용 가능합니다: [https://git.io/openseg](https://git.io/openseg) 및 [https://git.io/HRNet.OCR](https://git.io/HRNet.OCR).

우리는 객체-문맥 표현 스킴을 Transformer 인코더-디코더 프레임워크를 사용하여 다시 표현합니다. 객체 영역 학습과 객체 영역 표현 계산의 첫 두 단계는 디코더의 크로스-어텐션 모듈로 통합됩니다: 픽셀을 분류하여 객체 영역을 생성하는 데 사용되는 선형 투영은 카테고리 쿼리이며, 객체 영역 표현은 크로스-어텐션 출력입니다. 마지막 단계는 우리가 인코더에 추가한 크로스-어텐션 모듈이며, 여기서 키와 값은 디코더 출력이고 쿼리는 각 위치에서의 표현입니다. 자세한 내용은 3.3절에 제시되어 있습니다.


### 서론

의미론적 분할은 이미지의 각 픽셀에 클래스 레이블을 할당하는 문제입니다. 이는 컴퓨터 비전에서 기본적인 주제이며 자율 주행과 같은 다양한 실용적인 작업에 중요합니다. FCN[47] 이후로 깊은 컨볼루션 네트워크가 주된 해결책이었습니다. 고해상도 표현 학습[7,54], 이 논문에서 다루는 문맥 집계[80,6] 등 다양한 연구가 진행되었습니다. 한 위치의 문맥은 일반적으로 일련의 위치들, 예를 들어 주변 픽셀을 의미합니다. 초기 연구는 주로 문맥의 공간적 범위, 즉 공간적 스케일에 관한 것이었습니다. ASPP[6]와 PPM[80]과 같은 대표적인 작업들은 다중 스케일 문맥을 활용합니다. 최근에는 DANet[18], CFNet[77] 및 OCNet[72,71]과 같은 연구들이 위치와 그 문맥 위치 간의 관계를 고려하고, 유사한 표현에 더 높은 가중치를 부여하여 문맥 위치의 표현을 집계합니다.
![[스크린샷 2023-11-07 오후 4.13.45.png]]
> 그림 1: 우리의 OCR 스킴의 효과를 설명합니다. GT-OCR은 지상 진리를 활용하여 이상적인 객체-문맥 표현을 추정하며, 이는 우리 방법의 상한선입니다. OCR은 우리가 제안한 객체-문맥 표현의 성능을 보고합니다. 세 가지 방법, 기본선(Baseline), OCR 및 GT-OCR은 출력 스트라이드가 8인 확장된 ResNet-101을 백본으로 사용합니다. 우리는 그들의 (단일 스케일) 분할 결과를 Cityscapes val, ADE20K val, PASCAL-Context test 및 COCO-Stuff test에서 각각 평가합니다.
> (이 차트는 Cityscapes, ADE20K, PASCAL-Context, COCO-Stuff의 네 가지 다른 데이터셋에서 세 가지 의미론적 분할 방법: 기본선(Baseline), OCR(객체-문맥 표현), GT-OCR(지상 진리-OCR)의 성능을 비교합니다. 사용된 성능 메트릭은 mIoU(%)로, 의미론적 분할 작업에 대한 평가에서 흔히 사용되는 평균 교차 합집합 비율을 의미합니다.
> 차트에서 주목할 점은 다음과 같습니다:
> - 지상 진리 데이터를 사용하여 이상적인 객체-문맥 표현을 추정하는 GT-OCR은 모든 데이터셋에서 가장 높은 mIoU 점수를 보여줍니다. 이는 성능의 상한선 벤치마크로 작용함을 나타냅니다.
> - 연구에서 제안된 방법인 OCR은 기본선 방법보다 일관되게 성능이 더 우수하여, OCR 방법이 의미론적 분할에 있어 기본선 접근법보다 더 효과적임을 시사합니다.
> - OCR이 기본선보다 더 나은 개선을 보인 것은 Cityscapes와 ADE20K 데이터셋에서 가장 두드러집니다.
>  이는 OCR 접근법이 의미론적 분할 작업에서 기본선에 비해 상당한 성능 이점을 제공함을 보여줍니다.)

우리는 위치와 그 문맥 사이의 관계를 탐구하는 라인을 따라 문맥 표현 스킴을 조사할 것을 제안합니다. 동기는 한 픽셀에 할당된 클래스 레이블이 그 픽셀이 속한 객체의 카테고리라는 것입니다. 우리는 해당 클래스의 객체 영역 표현을 활용하여 한 픽셀의 표현을 증강하는 것을 목표로 합니다. 그림 1에 표시된 실증적 연구는, 지상 진리 객체 영역이 주어졌을 때, 이러한 표현 증강 스킴이 분할 품질을 크게 향상시킨다는 것을 검증합니다.

우리의 접근법은 세 가지 주요 단계로 구성됩니다. 첫째, 우리는 컨텍스트 픽셀들을 각각 클래스에 해당하는 소프트 객체 영역의 집합으로 나눕니다. 즉, 심층 네트워크(예: ResNet [25] 또는 HRNet [54])에서 계산된 거친 소프트 세분화를 통해 이러한 분할을 합니다. 이러한 분할은 ground-truth 세분화의 감독 하에 학습됩니다. 둘째, 우리는 각 객체 영역에 해당하는 픽셀의 표현을 집계함으로써 각 객체 영역의 표현을 추정합니다. 마지막으로, 우리는 각 픽셀의 표현을 객체 컨텍스트 표현(OCR)으로 보강합니다. OCR은 픽셀과 객체 영역 간의 관계에 따라 계산된 가중치로 모든 객체 영역 표현의 가중 집계입니다. 제안된 OCR 접근법은 전통적인 멀티스케일 컨텍스트 스키마와 다릅니다. 우리의 OCR은 동일한 객체 클래스의 컨텍스트 픽셀들을 다른 객체 클래스의 컨텍스트 픽셀들과 구별하는 반면, 멀티스케일 컨텍스트 스키마(예: ASPP [6] 및 PPM [80])는 그렇지 않고 단지 공간적 위치가 다른 픽셀들을 구별할 뿐입니다. 그림 2는 우리의 OCR 컨텍스트와 멀티스케일 컨텍스트의 차이점을 설명하는 예를 제공합니다. 한편, 우리의 OCR 접근법은 이전의 관계적 컨텍스트 스키마 [64,18,72,75,77]와도 다릅니다. 우리의 접근법은 컨텍스트 픽셀들을 객체 영역으로 구조화하고 픽셀과 객체 영역 간의 관계를 활용합니다. 반면, 이전의 관계적 컨텍스트 스키마는 컨텍스트 픽셀들을 개별적으로 고려하고 픽셀과 컨텍스트 픽셀 간의 관계만을 활용하거나 [18,72,77], 영역을 고려하지 않고 픽셀로부터만 관계를 예측합니다 [75].

우리는 다양한 도전적인 의미 분할 벤치마크에서 우리의 접근법을 평가합니다. 우리의 접근법은 PSPNet, DeepLabv3와 같은 멀티스케일 컨텍스트 스키마와 최근의 관계적 컨텍스트 스키마, 예를 들어 DANet를 능가하며 효율성도 향상되었습니다. 우리의 접근법은 다섯 가지 벤치마크에서 경쟁력 있는 성능을 달성했습니다: Cityscapes 테스트에서 84.5%, ADE20K val에서 45.66%, LIP val에서 56.65%, PASCAL-Context 테스트에서 56.2% 그리고 COCOStuff 테스트에서 40.5%. 또한, 우리는 우리의 접근법을 Panoptic-FPN [30]으로 확장하고 COCO 전경 분할 과제에서 우리의 OCR의 효과성을 검증합니다. 예를 들어, Panoptic-FPN + OCR은 COCO val에서 44.2%를 달성합니다.

### Related Work

멀티스케일 컨텍스트에 대해, PSPNet [80]은 피라미드 풀링 표현에 정규 합성곱을 수행하여 멀티스케일 컨텍스트를 포착합니다. DeepLab 시리즈 [5,6]는 다른 확장 비율을 가진 병렬 확장 합성곱을 채택합니다(각 비율은 다른 스케일의 컨텍스트를 포착합니다). 최근 작업들 [24,68,84,72]은 다양한 확장을 제안합니다. 예를 들어, DenseASPP [68]는 더 넓은 스케일 범위를 커버하기 위해 확장 비율을 조밀하게 만듭니다. 다른 연구들 [7,42,19]은 멀티스케일 컨텍스트로서 다중 해상도 특성을 활용하기 위해 인코더-디코더 구조를 구축합니다.
![[스크린샷 2023-11-07 오후 4.44.59.png]]
> 그림 2: ASPP를 예로 들어 멀티스케일 컨텍스트와 ■로 표시된 픽셀에 대한 OCR 컨텍스트를 설명합니다. (a) ASPP: 컨텍스트는 ■, ■로 표시된 희박하게 샘플링된 픽셀들의 집합입니다. 서로 다른 색상의 픽셀들은 다른 확장 비율에 해당합니다. 이 픽셀들은 객체 영역과 배경 영역 양쪽에 분포합니다. (b) 우리의 OCR: 컨텍스트는 객체 내에 있는 픽셀들의 집합(파란색으로 표시됨)으로 예상됩니다. 이미지는 ADE20K에서 선택되었습니다.

관계적 컨텍스트에 관해, DANet [18], CFNet [77] 및 OCNet [72,71]은 모든 픽셀로 구성된 컨텍스트의 픽셀 표현을 집계하여 각 픽셀의 표현을 증가시킵니다. 전역 컨텍스트 [46]와는 다르게, 이 작업들은 픽셀 간의 관계(또는 유사성)를 고려하는데, 이는 자기 주의(self-attention) 스키마 [64,61]에 기반하고 유사성을 가중치로 사용하여 가중 집계를 수행합니다. Double Attention 및 관련 작업 [8,75,9,40,38,74,35,26]과 ACFNet [75]은 픽셀을 일련의 영역으로 그룹화한 다음, 픽셀 표현을 사용하여 예측된 컨텍스트 관계를 고려하여 영역 표현을 집계함으로써 픽셀 표현을 증가시킵니다. 우리의 접근법은 관계적 컨텍스트 접근법이며 Double Attention 및 ACFNet과 관련이 있습니다. 차이점은 영역 형성과 픽셀-영역 관계 계산에 있습니다. 우리의 접근법은 ground-truth 세분화의 감독하에 영역을 학습합니다. 반면, ACFNet을 제외한 이전 접근법에서의 영역들은 무감독으로 형성됩니다. 한편, 픽셀과 영역 간의 관계는 픽셀과 영역 표현 모두를 고려하여 계산되는 반면, 이전 작업에서의 관계는 오직 픽셀 표현에서만 계산됩니다.

대략적인 분할에서 세밀한 분할로 점진적으로 세분화 지도를 개선하기 위해 다양한 대략적-세밀한 분할 스키마들이 개발되었습니다 [17,20,34,59,28,33,85]. 예를 들어, [34]는 대략적인 분할 지도를 추가적인 표현으로 간주하고 이를 원본 이미지 또는 다른 표현과 결합하여 세밀한 분할 지도를 계산합니다. 어떤 의미에서 우리의 접근법도 대략적-세밀한 스키마로 간주될 수 있습니다. 차이점은 우리가 대략적인 분할 지도를 추가적인 표현으로 직접 사용하는 대신 컨텍스트 표현을 생성하는 데 사용한다는 점입니다. 우리는 부가 자료에서 전통적인 대략적-세밀한 스키마와 우리의 접근법을 비교합니다.

영역별 세분화에 대해, 픽셀을 일련의 영역(보통 슈퍼픽셀)으로 조직화하고 각 영역을 분류하여 이미지 세분화 결과를 얻는 많은 영역별 세분화 방법들 [1,2,23,22,65,50,2,60]이 존재합니다. 우리의 접근법은 세분화를 위해 각 영역을 분류하지 않고 대신 픽셀에 대한 더 나은 표현을 학습하는 데 영역을 사용하며, 이는 더 나은 픽셀 라벨링으로 이어집니다.


### 3 Approach

의미 분할은 이미지 I의 각 픽셀 pi에 하나의 라벨 li를 할당하는 문제입니다. 여기서 li는 K개의 다른 클래스 중 하나입니다.

#### 3.1 Background



관계적 컨텍스트. 관계적 컨텍스트 스키마 [18,72,77]는 관계를 고려하여 각 픽셀에 대한 컨텍스트를 계산합니다: 여기서 I는 이미지 내 픽셀의 집합을 가리키고, w_is는 x_i와 x_s 사이의 관계를 나타내며, 이는 x_i로부터만 예측되거나 x_i와 x_s 양쪽에서 계산될 수 있습니다. δ(·)와 ρ(·)는 자기주의 [61]에서 수행되는 것처럼 두 가지 다른 변환 함수입니다. 전역 컨텍스트 스키마 [46]는 w_is = 1∣�∣∣I∣1​인 관계적 컨텍스트의 특별한 경우입니다.

#### 3.2 Formulation




#### 3.3 Segmentation Transformer: Rephrasing the OCR Method

인코더 크로스-어텐션. 인코더 크로스-어텐션 모듈(이후의 FFN과 함께)은 식 3에서 보여지는 것처럼 객체 영역 표현을 집계하는 역할을 합니다. 쿼리는 각 위치에서의 이미지 특징이며, 키와 값은 디코더 출력입니다. 식 5는 식 7의 어텐션 계산 방식과 같은 방식으로 가중치를 계산하고, 컨텍스트 집계 식 3은 식 8과 같으며, ρ(·)는 FFN 연산자에 해당합니다.

클래스 임베딩 및 클래스 어텐션 [14,58]과의 연결. 카테고리 쿼리는 비전 트랜스포머(ViT) [14]의 클래스 임베딩 및 이미지 트랜스포머(CaiT) [58] 내의 클래스-어텐션과 밀접합니다. 우리는 모든 클래스를 위한 통합된 임베딩 대신 각 클래스를 위한 임베딩을 가지고 있습니다. 세그멘테이션 트랜스포머에서의 디코더 크로스 어텐션은 CaiT [58]의 클래스 어텐션과 유사합니다. 인코더 및 디코더 아키텍처는 클래스 임베딩과 이미지 특징 모두에 대한 ViT 내의 자기주의(self-attention)와 밀접합니다. 두 크로스-어텐션과 두 자기주의가 동시에 수행되는 경우(그림 5에서 묘사된 바와 같이), 이는 단일 자기주의와 동일합니다. ImageNet 사전 학습 단계에서 카테고리 쿼리를 위한 어텐션 매개변수를 학습하는 것이 흥미롭습니다.



OCNet과 교차 자기주의(self-attention) [71]와의 연결. OCNet [71]은 자기주의를 활용합니다(즉, 그림 4에는 오직 인코더 자기주의 유닛만 포함되어 있고, 인코더 크로스-어텐션 유닛과 디코더는 포함되어 있지 않습니다). 자기주의 유닛은 교차된 자기주의 유닛에 의해 가속화됩니다. 이는 지역 자기주의와 전역 자기주의로 구성되며, 이는 지역 창(window) 위의 풀링된 특징들에 대한 자기주의로 간소화될 수 있습니다. 대안적인 스키마로서, 그림 4의 카테고리 쿼리는 모델 매개변수로 학습되는 것이 아니라 정규적으로 샘플링되거나 적응적으로 풀링된 이미지 특징으로 대체될 수 있습니다.

#### 3.4 Architecture


#### 3.5 Empirical Analysis

우리는 Cityscapes val에서 백본으로 딜레이티드 ResNet-101을 사용하여 경험적 분석 실험을 수행합니다.

객체 영역 감독. 우리는 객체 영역 감독의 영향을 연구합니다. 우리는 소프트 객체 영역에 대한 감독(즉, 손실)을 제거하고(그림 3의 분홍색 점선 박스 내부), ResNet-101의 stage-3에 다른 보조 손실을 추가하는 방식으로 접근법을 수정합니다. 우리는 모든 다른 설정을 동일하게 유지하고 표 1의 가장 왼쪽 2열에 결과를 보고합니다. 우리는 객체 영역을 형성하기 위한 감독이 성능에 매우 중요하다는 것을 볼 수 있습니다.

픽셀-영역 관계. 우리는 픽셀-영역 관계를 추정하기 위해 영역 표현을 사용하지 않는 다른 두 가지 메커니즘과 접근법을 비교합니다: (i) Double-Attention [8]은 픽셀 표현을 사용하여 관계를 예측합니다; (ii) ACFNet [75]은 중간 분할 지도를 직접 사용하여 관계를 나타냅니다. 우리는 위 두 메커니즘을 DA 스키마와 ACF 스키마로 표현합니다. 우리는 두 방법 모두를 직접 구현하고 멀티스케일 컨텍스트를 사용하지 않고 딜레이티드 ResNet-101만을 백본으로 사용합니다(ACFNet의 결과는 ASPP [75]를 사용하여 향상됩니다).]]

### 4 Experiments: Semantic Segmentation

#### 4.1 Datasets

Cityscapes. Cityscapes 데이터셋 [11]은 도시 장면 이해를 위한 과제입니다. 총 30개의 클래스가 있으며, 파싱 평가를 위해 19개의 클래스만 사용됩니다. 이 데이터셋은 5,000개의 고화질 픽셀 수준으로 정밀하게 주석이 달린 이미지와 20,000개의 대략적으로 주석이 달린 이미지를 포함하고 있습니다. 정밀하게 주석이 달린 5,000개의 이미지는 훈련, 검증, 테스트를 위해 2,975/500/1,525 이미지로 나뉩니다.

ADE20K. ADE20K 데이터셋 [82]은 2016년 ImageNet 장면 파싱 챌린지에 사용됩니다. 150개의 클래스와 1,038개의 이미지 수준 라벨을 가진 다양한 장면들이 있습니다. 이 데이터셋은 훈련, 검증, 테스트를 위해 20,000/2,000/3,000 이미지로 나뉩니다.

LIP. LIP 데이터셋 [21]은 2016년 LIP 챌린지에 단일 인간 파싱 과제로 사용됩니다. 대략 50,000개의 이미지와 20개의 클래스(19개의 의미론적 인간 부분 클래스와 1개의 배경 클래스)가 있습니다. 훈련, 검증, 테스트 세트는 각각 30,000, 10,000, 10,000 이미지로 구성됩니다.

PASCAL-Context. PASCAL-Context 데이터셋 [49]은 59개의 의미론적 클래스와 1개의 배경 클래스를 포함하는 도전적인 장면 파싱 데이터셋입니다. 훈련 세트와 테스트 세트는 각각 4,998개와 5,105개의 이미지로 구성됩니다.

COCO-Stuff. COCO-Stuff 데이터셋 [3]은 171개의 의미론적 클래스를 포함하는 도전적인 장면 파싱 데이터셋입니다. 훈련 세트와 테스트 세트는 각각 9,000개와 1,000개의 이미지로 구성됩니다.

#### 4.2 Implementation Details


재현된 접근법들에 대한 훈련 설정, 예를 들어 PPM, ASPP 등을 보면 공정성을 확보하기 위해 이전 연구들 [6,76,80]에 따라 벤치마크 데이터셋에 대한 훈련 설정을 따릅니다.

Cityscapes: 초기 학습률을 0.01, 가중치 감소를 0.0005, 크롭 크기를 769 × 769, 배치 크기를 기본적으로 8로 설정합니다. val/test 세트에서 평가된 실험들에 대해, 우리는 훈련/훈련+검증 세트에서 각각 40K/100K 훈련 반복을 설정합니다. 추가 데이터로 증강된 실험들에 대해서: (i) 코스 데이터와 함께, 우리는 먼저 100K 반복 동안 훈련 + 검증 세트에서 0.01의 초기 학습률로 모델을 훈련시킨 다음, 코스 세트에서 50K 반복 동안 모델을 미세 조정하고 훈련+검증에서 20K 반복 동안 동일한 초기 학습률 0.001로 모델을 계속 미세 조정합니다. (ii) 코스 + Mapillary [50]와 함께, 우리는 먼저 Mapillary 훈련 세트에서 배치 크기 16과 초기 학습률 0.01로 500K 반복 동안 모델을 사전 훈련합니다(이는 Mapillary 검증 세트에서 50.8%를 달성합니다), 그런 다음 Cityscapes에서 훈련 + 검증(100K 반복) → 코스(50K 반복) → 훈련 + 검증(20K 반복)의 순서로 모델을 미세 조정합니다. Cityscapes에서 위 세 단계의 미세 조정 동안 초기 학습률을 0.001, 배치 크기를 8로 설정합니다.

ADE20K: 초기 학습률을 0.02, 가중치 감소를 0.0001, 크롭 크기를 520 × 520, 배치 크기를 16, 훈련 반복을 150K로 설정합니다(특별한 명시가 없는 경우).

LIP: 초기 학습률을 0.007, 가중치 감소를 0.0005, 크롭 크기를 473×473, 배치 크기를 32, 훈련 반복을 100K로 설정합니다(특별한 명시가 없는 경우).

PASCAL-Context: 초기 학습률을 0.001, 가중치 감소를 0.0001, 크롭 크기를 520 × 520, 배치 크기를 16, 훈련 반복을 30K로 설정합니다(특별한 명시가 없는 경우).

COCO-Stuff: 초기 학습률을 0.001, 가중치 감소를 0.0001, 크롭 크기를 520 × 520, 배치 크기를 16, 훈련 반복을 60K로 설정합니다(특별한 명시가 없는 경우).


#### 4.3 Comparison with Existing Context Schemes

우리는 백본으로 딜레이티드 ResNet-101을 사용하고, 공정성을 보장하기 위해 동일한 훈련/테스트 설정을 사용하여 실험을 수행합니다.

멀티스케일 컨텍스트. 우리는 표 2에서 Cityscapes 테스트, ADE20K val 및 LIP val을 포함한 세 가지 벤치마크에서 PPM [80] 및 ASPP [6]을 포함한 멀티스케일 컨텍스트 스키마와 OCR을 비교합니다. 우리가 재현한 PPM/ASPP는 [80,6]에서 처음 보고된 수치들보다 뛰어납니다. 표 2에서 볼 수 있듯이, 우리의 OCR은 멀티스케일 컨텍스트 스키마들을 큰 차이로 능가합니다. 예를 들어, OCR이 PPM(ASPP)을 넘어서는 절대적인 이득은 네 가지 비교에 대해 각각 1.5%(0.8%), 0.8%(0.7%), 0.78%(0.68%), 0.84%(0.5%)입니다. 우리가 알기로는, 이러한 개선은 기반 모델들(딜레이티드 ResNet-101을 사용한)이 이미 강력하고 우리 OCR의 복잡성이 훨씬 작음에도 불구하고 상당한 것입니다.

관계적 컨텍스트. 우리는 자기 주의(Self-Attention) [61,64], Criss-Cross 주의(CC-Attention) [27], DANet [18] 및 Double Attention [8]을 포함한 다양한 관계적 컨텍스트 스키마와 동일한 세 가지 벤치마크에서 OCR을 비교합니다.


> 이 표는 다양한 벤치마크에서 OCR이 멀티스케일 컨텍스트 스키마(PPM과 ASPP)와 관계적 컨텍스트 스키마(예: CC-Attention, DANet, Self-Attention, Double Attention)를 일관되게 능가하는 것을 보여줍니다. OCR은 Cityscapes val을 훈련 데이터로 사용하지 않고도, 공정한 비교 하에 여러 벤치마크에서 뛰어난 성능을 보여주었습니다. 예를 들어, Cityscapes에서 OCR은 PPM 및 ASPP가 보고된 수치보다 각각 3.8%, 1.7% 높은 성능을 보여줍니다. ADE20K와 LIP에서도 OCR은 다른 모든 방법보다 더 높은 정확도를 달성했습니다.
> 
> 표 3에서는 OCR이 Cityscapes와 ADE20K, LIP에서 다른 관계적 컨텍스트 스키마보다 일관되게 더 나은 성능을 내는 것을 볼 수 있습니다. 특히, Double Attention은 영역 수의 선택에 민감하며, 최상의 성능을 보고하기 위해 이 하이퍼파라미터를 64로 세밀하게 조정했습니다.
> 
> 표 4는 복잡성 비교를 보여줍니다. OCR은 다른 방법들과 비교하여 가장 적은 GPU 메모리와 가장 짧은 실행 시간을 필요로 합니다. 예를 들어, OCR은 PPM과 ASPP가 사용하는 메모리의 거의 절반만을 사용하며, 실행 시간도 각각 99ms와 97ms 대비하여 45ms로 상당히 줄였습니다.
> 
> 이 표를 발표 자료에서 설명할 때, OCR의 효율성과 높은 성능을 강조하는 것이 중요합니다. OCR이 기존의 방법들을 성능뿐만 아니라 계산 효율성 측면에서도 능가한다는 점을 명확히 전달하면 됩니다.


표 3에 나타난 결과에 따르면, 우리의 OCR은 공정한 비교 하에 이러한 관계적 컨텍스트 스키마들을 능가합니다. 특히, 우리 OCR의 복잡성은 다른 대부분의 방법들보다 훨씬 작습니다.

복잡성. 우리는 OCR의 효율성을 멀티스케일 컨텍스트 스키마와 관계적 컨텍스트 스키마의 효율성과 비교합니다. 우리는 컨텍스트 모듈들에 의해 도입된 추가적인 매개변수, GPU 메모리, 계산 복잡도(실행에 필요한 FLOPs의 수로 측정) 및 추론 시간을 측정하며, 백본에서 오는 복잡성은 계산에 포함하지 않습니다. 표 4의 비교는 제안된 OCR 스키마의 우수성을 보여줍니다.

매개변수: 대부분의 관계적 컨텍스트 스키마는 멀티스케일 컨텍스트 스키마에 비해 적은 매개변수를 요구합니다. 예를 들어, 우리의 OCR은 PPM과 ASPP에 비해 각각 1/2, 2/3의 매개변수만을 필요로 합니다.

메모리: 우리의 OCR과 Double Attention은 다른 접근법들(예: DANet, PPM)과 비교하여 훨씬 적은 GPU 메모리를 필요로 합니다. 예를 들어, 우리의 GPU 메모리 소비는 PPM, DANet, CC-Attention, Self-Attention에 비해 각각 1/4, 1/10, 1/2, 1/10입니다.

FLOPs: 우리의 OCR은 PPM, ASPP, DANet, CC-Attention, Self-Attention에 비해 각각 1/2, 7/10, 3/10, 2/5, 1/2의 FLOPs만을 필요로 합니다.

실행 시간: OCR의 실행 시간은 매우 짧습니다: PPM, ASPP, DANet, CC-Attention, Self-Attention에 비해 각각 1/2, 1/2, 1/3, 1/3, 1/2만큼밖에 되지 않습니다.

일반적으로, 성능, 메모리 복잡도, GFLOPs 및 실행 시간의 균형을 고려할 때 우리의 OCR은 훨씬 나은 선택입니다.
#### 4.4 Comparison with the State-of-the-Art

연구들은 기본적인 베이스라인과 고급 베이스라인을 적용하는 두 그룹으로 분류됩니다. 기본적인 베이스라인은 ResNet-101을 사용하며, 고급 베이스라인은 PSPNet, DeepLabv3, Multi-grid(MG)와 같이 더 강력한 백본을 가진 구조들입니다. 저자들은 자신들의 OCR(Object Contextual Representations) 방법을 간단한 베이스라인과 고급 베이스라인(ResNet-101 대비 HRNet-W48)에 적용하여 각각의 향상도를 비교합니다. Cityscapes 데이터셋에 대한 실험에서 저자들의 방법은 기본 베이스라인을 사용하는 다른 방법들과 비교하여 최고의 성능(81.8%)을 달성했으며, 고급 베이스라인을 사용하는 방법들과 비교하여도 높은 성능을 보였습니다. 특히, HRNet-W48에 OCR을 적용하고 Mapillary 데이터셋에서 사전 훈련을 거친 후 Cityscapes 테스트에서 84.2%의 성능을 달성했습니다. 추가적으로 경계 품질을 개선하는 새로운 후처리 방법 SegFix를 적용하여 0.3%의 성능 향상을 이루었고, 최종적으로 'HRNet + OCR + SegFix' 방법은 제출 당시 Cityscapes 리더보드에서 1위를 차지했습니다. 저자들은 HRNet-W48에 PPM과 ASPP를 별도로 적용해봤으나 성능 향상이 없었고, OCR은 일관되게 성능을 향상시켰다고 언급합니다. 최근 연구 중 하나는 'HRNet + OCR'에 새로운 계층적 멀티스케일 주의 메커니즘을 결합하여 Cityscapes 리더보드에서 새로운 최고 성능 85.4%를 달성했다고 합니다.

ADE20K 데이터셋에서, 저자들의 OCR은 간단한 베이스라인과 고급 베이스라인을 기반으로 하는 대부분의 이전 방법들과 비교하여 경쟁력 있는 성능(45.28%와 45.66%)을 달성했습니다. 예를 들어, ACFNet은 다중 스케일 맥락과 관계 맥락을 활용하여 더 높은 성능을 달성했고, 최근의 ACNet은 더 풍부한 지역 및 전역 맥락을 결합하여 최고의 성능을 달성했습니다.

LIP 데이터셋에서는 저자들의 접근법이 기본 베이스라인을 기반으로 할 때 55.60%의 가장 좋은 성능을 달성했으며, 강력한 백본인 HRNetV2-W48을 적용함으로써 성능이 56.65%로 향상되어 이전의 접근법들을 능가했습니다. 최근의 CNIF 방법은 인간 부위의 계층적 구조 지식을 주입함으로써 56.93%의 최고 성능을 달성했습니다. 저자들의 방법도 이러한 계층적 구조 지식으로부터 이점을 얻을 수 있습니다. 모든 결과는 다중 스케일 테스팅 없이 단지 뒤집기(flip) 테스팅만을 기반으로 합니다.

PASCAL-Context 데이터셋에서는 저자들이 59개 카테고리에서 성능을 평가하였고, 그들의 방법이 간단한 베이스라인을 기반으로 하는 이전의 최고 방법들과 고급 베이스라인을 기반으로 하는 이전의 최고 방법들을 모두 능가하는 것으로 나타났습니다. HRNet-W48 + OCR 접근법은 56.2%의 최고 성능을 달성하여, 예를 들어 ACPNet(54.7%)와 ACNet(54.1%) 같은 두 번째로 좋은 방법들을 크게 능가했습니다.

COCO-Stuff 데이터셋에서는 저자들의 방법이 ResNet-101을 기반으로 할 때 39.5%, HRNetV2-48을 기반으로 할 때 40.5%의 최고 성능을 달성했습니다.

질적 결과에 대해서는 페이지 수 제한으로 인해 부록 자료에서 설명합니다.


### 5 Experiments: Panoptic Segmentation

전체 장면 분할 과제는 개별 인스턴스 분할과 의미 분할을 통합합니다.

데이터셋으로, 저자들은 COCO 데이터셋을 선택하여 전체 장면 분할에서의 방법의 효과를 연구합니다. 이들은 이전 연구를 따라 2017년 COCO 이미지를 사용하며, 이 이미지들은 80개의 'thing' 클래스와 53개의 'stuff' 클래스로 주석이 달려 있습니다.

훈련 세부사항에 대해, 저자들은 Detectron2의 “COCO 전체 장면 분할 기본선(BaseLines) with Panoptic FPN (3× 학습 일정)” 설정을 따릅니다. 재현된 Panoptic FPN은 원래 논문의 성능보다 더 높은 성능을 달성했고(ResNet-50을 사용한 Panoptic FPN, PQ: 39.2% / ResNet-101을 사용한 Panoptic FPN, PQ: 40.3%), 저자들은 이 더 높은 재현 결과를 자신들의 베이스라인으로 선택합니다.

구현에서, 저자들은 Panoptic-FPN 내의 의미 분할 헤드로부터 나온 원래 예측을 사용하여 부드러운 객체 영역을 계산하고, OCR 헤드를 사용하여 정제된 의미 분할 맵을 예측합니다. 의미 분할 헤드와 OCR 헤드 모두에 대해 0.25의 손실 가중치를 설정합니다. 공정한 비교를 위해 다른 모든 훈련 설정은 동일하게 유지됩니다. 의미 분할 작업용으로 이미 구현된 OCR을 어떠한 조정도 없이 직접 사용합니다.

결과에서, 표 6을 통해 OCR이 Panoptic-FPN (ResNet-101)의 PQ 성능을 43.0%에서 44.2%로 개선하는 것을 볼 수 있습니다. 주된 개선 사항은 mIoU와 PQSt로 측정된 'stuff' 영역의 더 나은 분할 품질에서 나옵니다. 구체적으로, OCR은 Panoptic-FPN (ResNet-101)의 mIoU와 PQSt를 각각 1.0%와 2.3% 향상시켰습니다. 일반적으로, “Panoptic-FPN + OCR”의 성능은 다양한 최신 방법들과 매우 경쟁력이 있습니다. 저자들은 또한 PPM과 ASPP로 Panoptic-FPN의 결과를 보고하고 부록 자료에서 OCR의 장점을 설명합니다.


### 6 Conclusions

이 작업에서는 의미 분할을 위한 객체-컨텍스트 표현 접근법을 제시합니다. 이 접근법의 성공의 주된 이유는 픽셀의 라벨이 그 픽셀이 속한 객체의 라벨이며, 각 픽셀을 해당 객체 영역 표현으로 특징짓는 것으로 픽셀 표현이 강화되기 때문입니다. 우리는 우리의 접근법이 다양한 벤치마크에서 일관된 개선을 가져온다는 것을 경험적으로 보여줍니다.